{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import docx\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一键操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Done!: used 1.19s--------\n",
      "\n",
      "\n",
      "\n",
      "---Getting URL...---\n",
      "-----URL 36260656.html used 0.91s-----\n",
      "---Fetched the contents---\n",
      "没更新\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.xbiquge.cc/book/49549/35899003.html' #测试用 章节\n",
    "main_url =  'https://www.xbiquge.cc/book/49549/'       #豪婿\n",
    "cache = '36254785.html'\n",
    "#main_url = 'https://www.xbiquge.cc/book/420/'           #龙王传说\n",
    "# main_url = 'https://www.xbiquge.cc/book/7177/'          #阴阳冕\n",
    "#main_url ='https://www.xbiquge.cc/book/46844/'          #终极斗罗\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3314.0 Safari/537.36 SE 2.X MetaSr 1.0'\n",
    "}\n",
    "\n",
    "\n",
    "t = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        #加入timeout防止单次访问用时过长，timeout = 5防止对服务器造成过大负担\n",
    "        main_response = requests.get(main_url, headers = header, timeout = 5)\n",
    "        break\n",
    "    except:\n",
    "        print('---URL {} Failed...Try to connect again'.format(main_url))\n",
    "print('---------Done!: used {:.2f}s--------'.format(time.time() - t))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "#www.xbiquge.cc 使用gbk进行编码，所以需要gbk解码\n",
    "main_response.encoding = \"gbk\"\n",
    "main_text = main_response.content.decode('gbk')\n",
    "\n",
    "\n",
    "#html.parser是解析器，也可是lxml\n",
    "main_soup = BeautifulSoup(main_text,'lxml')\n",
    "#把整个网页转换成解析过的String，不过感觉不需要这一步\n",
    "#直接使用main_text可以达到同样效果\n",
    "strMain = main_soup.prettify()\n",
    "#获得主标题\n",
    "find_title = re.compile(r'<meta content=\"(.*?)\" property=\"og:title\"/>')\n",
    "#获得作者\n",
    "find_author = re.compile(r'<meta content=\"(.*?)\" property=\"og:novel:author\"/>')\n",
    "mainTitle = find_title.findall(strMain)[0]\n",
    "author = find_author.findall(strMain)[0]\n",
    "\n",
    "#解析主网页的小说列表，笔趣阁都是以同一个形式加载的\n",
    "pattern = re.compile(r'<a href=\"(\\d+.html)\">')\n",
    "text_list = pattern.findall(main_soup.prettify())\n",
    "\n",
    "\n",
    "def write_to_doc(book):\n",
    "    begin_time = time.time()\n",
    "    chapter_url = main_url+book\n",
    "    print('---Getting URL...---')\n",
    "    while True:\n",
    "        try:\n",
    "            #加timeout防单词用时过长\n",
    "            chapter_response = requests.get(chapter_url,headers = header,timeout = 5)\n",
    "            break\n",
    "        except:\n",
    "            print('---URL {} Failed...Try to connect again'.format(book))\n",
    "            continue\n",
    "    print('-----URL {} used {:.2f}s-----'.format(book, time.time() - begin_time))\n",
    "    #print(chapter_response.content)\n",
    "    #小说主体部分需要重新解码\n",
    "#     try:\n",
    "    chapter_response.encoding = \"gbk\"\n",
    "    chapter_text = chapter_response.content.decode('gbk')\n",
    "#     except:\n",
    "#         print(\"Missing URL:{}\".format(book))\n",
    "#         return \"Missing URL:{}\".format(book)\", \"Missing URL:{}\".format(book)\"\n",
    "    #以beautiful的形式读入，方便后续解析\n",
    "    chapter_soup = BeautifulSoup(chapter_text,'lxml')\n",
    "\n",
    "    #通过分析页面，笔趣阁小说主体部分在 id为content的div里\n",
    "    div = chapter_soup.find_all('div',id='content')#,string='更多'\n",
    "    #将获得的tag们全部变成string\n",
    "    div_text =div[0].prettify()\n",
    "    #网页中每个段落之间会有<br/>，刚好适合用来分段落\n",
    "    content = div_text.split('<br/>')[1:]\n",
    "    #再将过多的\\n去除\n",
    "    content = \"\".join(content).split('\\n')[:-2]\n",
    "    \n",
    "    #获得章节名、文章名、以及来源\n",
    "    title = chapter_soup.title.text\n",
    "    mainTitle = title.split('_')[1]\n",
    "    source = title.split('_')[2]\n",
    "    chapterTitle = title.split('_')[0]\n",
    "    \n",
    "    print('---Fetched the contents---')\n",
    "    \n",
    "    return content, chapterTitle\n",
    "\n",
    "\n",
    "def download(text_list, mainTitle):\n",
    "    begin = time.time()\n",
    "    try:\n",
    "        f = open('{}Cache.txt'.format(mainTitle))\n",
    "        cache = f.read()\n",
    "        f.close()\n",
    "    except:\n",
    "        cache = None\n",
    "    \n",
    "    cacheMainTitle = mainTitle\n",
    "    if cache:\n",
    "        content, chapterTitle = write_to_doc(cache)\n",
    "        mainTitle = mainTitle + \"更新自\" + chapterTitle\n",
    "    if cache:\n",
    "        index = text_list.index(cache)\n",
    "        if len(text_list) - 1 == index:\n",
    "            print(\"没更新\")\n",
    "            return\n",
    "        text_list = text_list[index:]\n",
    "    #新建一个docx文件\n",
    "    file=docx.Document()\n",
    "    #新建一个txt文件\n",
    "    file_handle = open('{}.txt'.format(mainTitle), mode = 'w')\n",
    "    #写入主标题和作者名\n",
    "    file.add_heading(\"{}\".format(mainTitle),level=1)\n",
    "    file.add_heading(\"作者:{}\".format(author),level=2)\n",
    "    file.add_page_break()\n",
    "    count = 0\n",
    "    total = len(text_list)\n",
    "    \n",
    "    for i in text_list:\n",
    "        #count 用来计数，数字为章节数\n",
    "        count  += 1\n",
    "        print('-------------Doing chapter: {}/{}------------'.format(count, total))\n",
    "\n",
    "        content, chapterTitle = write_to_doc(i)\n",
    "        #写入段落标题\n",
    "        file.add_heading(\"{}\".format(chapterTitle),level=2)\n",
    "        file_handle.write('\\n')\n",
    "        file_handle.write(chapterTitle)\n",
    "        file_handle.write('\\n')\n",
    "        print('----begin to write chapter {} into doc----'.format(count))\n",
    "        #将爬取回的内容一段一段写入docx\n",
    "        for i in content:\n",
    "            if i !=  ' ':\n",
    "                file.add_paragraph(i)\n",
    "                file_handle.write(i)\n",
    "                file_handle.write('\\n')\n",
    "        print('---Saving for chapter {}!---'.format(count))\n",
    "        #加入分页，每一章的开头都是全新的一页纸\n",
    "        file.add_page_break()\n",
    "        #保存docx文件\n",
    "        file.save(\"{}.docx\".format(mainTitle)) #输出的word名字\n",
    "        #sleep(1)防止对服务器造成过大负担\n",
    "        time.sleep(1)\n",
    "        print('---Done for chapter: {}---\\n'.format(count))\n",
    "    file_handle.close()\n",
    "    f = open('{}Cache.txt'.format(cacheMainTitle), mode = \"w\")\n",
    "    f.write(text_list[-1])\n",
    "    print(\"---------Set cache--------\")\n",
    "    f.close()\n",
    "    print('--------------------------------DONE!!!!!!!-------------------------')\n",
    "    print('------------------uesd: {:.2f}s-------------------'.format(time.time() - begin))\n",
    "    \n",
    "download(text_list, mainTitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分布操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.xbiquge.cc/book/49549/35899003.html' #测试用 章节\n",
    "main_url =  'https://www.xbiquge.cc/book/49549/'       #豪婿\n",
    "cache = '36254785.html'\n",
    "#main_url = 'https://www.xbiquge.cc/book/420/'           #龙王传说\n",
    "# main_url = 'https://www.xbiquge.cc/book/7177/'          #阴阳冕\n",
    "#main_url ='https://www.xbiquge.cc/book/46844/'          #终极斗罗\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3314.0 Safari/537.36 SE 2.X MetaSr 1.0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Done!: used 1.42s--------\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        #加入timeout防止单次访问用时过长，timeout = 5防止对服务器造成过大负担\n",
    "        main_response = requests.get(main_url, headers = header, timeout = 5)\n",
    "        break\n",
    "    except:\n",
    "        print('---URL {} Failed...Try to connect again'.format(main_url))\n",
    "print('---------Done!: used {:.2f}s--------'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解码 乱码->中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#www.xbiquge.cc 使用gbk进行编码，所以需要gbk解码\n",
    "main_response.encoding = \"gbk\"\n",
    "main_text = main_response.content.decode('gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入beautifulsoup进行解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html.parser是解析器，也可是lxml\n",
    "main_soup = BeautifulSoup(main_text,'lxml')\n",
    "#把整个网页转换成解析过的String，不过感觉不需要这一步\n",
    "#直接使用main_text可以达到同样效果\n",
    "strMain = main_soup.prettify()\n",
    "#获得主标题\n",
    "find_title = re.compile(r'<meta content=\"(.*?)\" property=\"og:title\"/>')\n",
    "#获得作者\n",
    "find_author = re.compile(r'<meta content=\"(.*?)\" property=\"og:novel:author\"/>')\n",
    "mainTitle = find_title.findall(strMain)[0]\n",
    "author = find_author.findall(strMain)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取小说list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#解析主网页的小说列表，笔趣阁都是以同一个形式加载的\n",
    "pattern = re.compile(r'<a href=\"(\\d+.html)\">')\n",
    "text_list = pattern.findall(main_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['36251647.html',\n",
       " '36254783.html',\n",
       " '36254784.html',\n",
       " '36254785.html',\n",
       " '36257654.html',\n",
       " '36260652.html',\n",
       " '36260653.html',\n",
       " '36260654.html',\n",
       " '36260655.html',\n",
       " '36260656.html']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test block\n",
    "text_list[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 每一个章节分别进行提取输出操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 1. 读取每一个章节的内容和标题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_doc(book):\n",
    "    begin_time = time.time()\n",
    "    chapter_url = main_url+book\n",
    "    print('---Getting URL...---')\n",
    "    while True:\n",
    "        try:\n",
    "            #加timeout防单词用时过长\n",
    "            chapter_response = requests.get(chapter_url,headers = header,timeout = 5)\n",
    "            break\n",
    "        except:\n",
    "            print('---URL {} Failed...Try to connect again'.format(book))\n",
    "            continue\n",
    "    print('-----URL {} used {:.2f}s-----'.format(book, time.time() - begin_time))\n",
    "    #print(chapter_response.content)\n",
    "    #小说主体部分需要重新解码\n",
    "#     try:\n",
    "    chapter_response.encoding = \"gbk\"\n",
    "    chapter_text = chapter_response.content.decode('gbk')\n",
    "#     except:\n",
    "#         print(\"Missing URL:{}\".format(book))\n",
    "#         return \"Missing URL:{}\".format(book)\", \"Missing URL:{}\".format(book)\"\n",
    "    #以beautiful的形式读入，方便后续解析\n",
    "    chapter_soup = BeautifulSoup(chapter_text,'lxml')\n",
    "\n",
    "    #通过分析页面，笔趣阁小说主体部分在 id为content的div里\n",
    "    div = chapter_soup.find_all('div',id='content')#,string='更多'\n",
    "    #将获得的tag们全部变成string\n",
    "    div_text =div[0].prettify()\n",
    "    #网页中每个段落之间会有<br/>，刚好适合用来分段落\n",
    "    content = div_text.split('<br/>')[1:]\n",
    "    #再将过多的\\n去除\n",
    "    content = \"\".join(content).split('\\n')[:-2]\n",
    "    \n",
    "    #获得章节名、文章名、以及来源\n",
    "    title = chapter_soup.title.text\n",
    "    mainTitle = title.split('_')[1]\n",
    "    source = title.split('_')[2]\n",
    "    chapterTitle = title.split('_')[0]\n",
    "    \n",
    "    print('---Fetched the contents---')\n",
    "    \n",
    "    return content, chapterTitle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.写入word文档和同名txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download(text_list, mainTitle):\n",
    "    begin = time.time()\n",
    "    try:\n",
    "        f = open('{}Cache.txt'.format(mainTitle))\n",
    "        cache = f.read()\n",
    "        f.close()\n",
    "    except:\n",
    "        cache = None\n",
    "    \n",
    "    cacheMainTitle = mainTitle\n",
    "    if cache:\n",
    "        content, chapterTitle = write_to_doc(cache)\n",
    "        mainTitle = mainTitle + \"更新自\" + chapterTitle\n",
    "    if cache:\n",
    "        index = text_list.index(cache)\n",
    "        if len(text_list) - 1 == index:\n",
    "            print(\"没更新\")\n",
    "            return\n",
    "        text_list = text_list[index:]\n",
    "    #新建一个docx文件\n",
    "    file=docx.Document()\n",
    "    #新建一个txt文件\n",
    "    file_handle = open('{}.txt'.format(mainTitle), mode = 'w')\n",
    "    #写入主标题和作者名\n",
    "    file.add_heading(\"{}\".format(mainTitle),level=1)\n",
    "    file.add_heading(\"作者:{}\".format(author),level=2)\n",
    "    file.add_page_break()\n",
    "    count = 0\n",
    "    total = len(text_list)\n",
    "    \n",
    "    for i in text_list:\n",
    "        #count 用来计数，数字为章节数\n",
    "        count  += 1\n",
    "        print('-------------Doing chapter: {}/{}------------'.format(count, total))\n",
    "\n",
    "        content, chapterTitle = write_to_doc(i)\n",
    "        #写入段落标题\n",
    "        file.add_heading(\"{}\".format(chapterTitle),level=2)\n",
    "        file_handle.write('\\n')\n",
    "        file_handle.write(chapterTitle)\n",
    "        file_handle.write('\\n')\n",
    "        print('----begin to write chapter {} into doc----'.format(count))\n",
    "        #将爬取回的内容一段一段写入docx\n",
    "        for i in content:\n",
    "            if i !=  ' ':\n",
    "                file.add_paragraph(i)\n",
    "                file_handle.write(i)\n",
    "                file_handle.write('\\n')\n",
    "        print('---Saving for chapter {}!---'.format(count))\n",
    "        #加入分页，每一章的开头都是全新的一页纸\n",
    "        file.add_page_break()\n",
    "        #保存docx文件\n",
    "        file.save(\"{}.docx\".format(mainTitle)) #输出的word名字\n",
    "        #sleep(1)防止对服务器造成过大负担\n",
    "        time.sleep(1)\n",
    "        print('---Done for chapter: {}---\\n'.format(count))\n",
    "    file_handle.close()\n",
    "    f = open('{}Cache.txt'.format(cacheMainTitle), mode = \"w\")\n",
    "    f.write(text_list[-1])\n",
    "    print(\"---------Set cache--------\")\n",
    "    f.close()\n",
    "    print('--------------------------------DONE!!!!!!!-------------------------')\n",
    "    print('------------------uesd: {:.2f}s-------------------'.format(time.time() - begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36257654.html\n",
      "---Getting URL...---\n",
      "-----URL 36257654.html used 0.92s-----\n",
      "---Fetched the contents---\n",
      "-------------Doing chapter: 1/6------------\n",
      "---Getting URL...---\n",
      "-----URL 36257654.html used 0.87s-----\n",
      "---Fetched the contents---\n",
      "----begin to write chapter 1 into doc----\n",
      "---Saving for chapter 1!---\n",
      "---Done for chapter: 1---\n",
      "\n",
      "-------------Doing chapter: 2/6------------\n",
      "---Getting URL...---\n",
      "-----URL 36260652.html used 0.87s-----\n",
      "---Fetched the contents---\n",
      "----begin to write chapter 2 into doc----\n",
      "---Saving for chapter 2!---\n",
      "---Done for chapter: 2---\n",
      "\n",
      "-------------Doing chapter: 3/6------------\n",
      "---Getting URL...---\n",
      "-----URL 36260653.html used 0.93s-----\n",
      "---Fetched the contents---\n",
      "----begin to write chapter 3 into doc----\n",
      "---Saving for chapter 3!---\n",
      "---Done for chapter: 3---\n",
      "\n",
      "-------------Doing chapter: 4/6------------\n",
      "---Getting URL...---\n",
      "-----URL 36260654.html used 0.91s-----\n",
      "---Fetched the contents---\n",
      "----begin to write chapter 4 into doc----\n",
      "---Saving for chapter 4!---\n",
      "---Done for chapter: 4---\n",
      "\n",
      "-------------Doing chapter: 5/6------------\n",
      "---Getting URL...---\n",
      "-----URL 36260655.html used 0.91s-----\n",
      "---Fetched the contents---\n",
      "----begin to write chapter 5 into doc----\n",
      "---Saving for chapter 5!---\n",
      "---Done for chapter: 5---\n",
      "\n",
      "-------------Doing chapter: 6/6------------\n",
      "---Getting URL...---\n",
      "-----URL 36260656.html used 0.93s-----\n",
      "---Fetched the contents---\n",
      "----begin to write chapter 6 into doc----\n",
      "---Saving for chapter 6!---\n",
      "---Done for chapter: 6---\n",
      "\n",
      "---------Set cache--------\n",
      "--------------------------------DONE!!!!!!!-------------------------\n",
      "------------------uesd: 12.73s-------------------\n"
     ]
    }
   ],
   "source": [
    "download(text_list, mainTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "25.205666666666666\n"
     ]
    }
   ],
   "source": [
    "print(15127.34/60//60)\n",
    "print(1512.34/60%60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/test/test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-4310012e94a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfile_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/test/test.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Hello World'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Hello World'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/test/test.txt'"
     ]
    }
   ],
   "source": [
    "file_handle = open('/test/test.txt', mode = 'w')\n",
    "file_handle.write('Hello World')\n",
    "file_handle.write('\\n')\n",
    "file_handle.write('Hello World')\n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
