{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main URL area, contain for each novel \n",
    "url = \"https://www.xbiquge.so/book/49549/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import area\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the html text and retreive all the chapters \n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "# Since BiQuGe use cache in their website, we need to drop first 12 website\n",
    "# which are the newest 12 chapters and have been included later\n",
    "sub_url_list = soup.find(id='list').find_all('a')[12:]\n",
    "sub_url_list = [sub_url.get(\"href\") for sub_url in sub_url_list]\n",
    "title = str(soup.find(id='list').find(\"dt\")).split(\"》\")[0].split(\"《\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By checking the corresponding website to retrieve content\n",
    "def get_text_and_title(url, sub_url):\n",
    "#     sub_url = sub_url_list[0]\n",
    "    chapter_url = url + sub_url\n",
    "    r = requests.get(chapter_url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    sub_text = soup.find(id='content').text.split(\"\\xa0\\xa0\\xa0\\xa0\")[1:]\n",
    "    sub_title = soup.find(\"title\").text.split(\"_\")[0]\n",
    "    return sub_text, sub_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function used in multi-thread\n",
    "def get_list_text(selected_list):\n",
    "    # selected_list: list like, [[index of the chapter, url of the chapter], ...]\n",
    "    ret = []\n",
    "    for index, sub_url in selected_list:\n",
    "        sub_text, sub_title = get_text_and_title(url, sub_url)\n",
    "        ret.append([index, sub_title, sub_text])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "321\n",
      "321\n",
      "321\n",
      "321\n",
      "321\n",
      "320\n",
      "320\n",
      "320\n",
      "320\n"
     ]
    }
   ],
   "source": [
    "workers = 10\n",
    "indexed_list = [[index, i] for index, i in enumerate(sub_url_list)]\n",
    "n_workers = list(np.array_split(indexed_list, workers))\n",
    "for i in n_workers:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
